{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Relief Web API fetch reports incrementaly"
      ],
      "metadata": {
        "id": "vn8hHAsYZ-Bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import requests\n",
        "import pandas as pd\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "Eh4PjRoCxOoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# File to store last fetched offset\n",
        "OFFSET_FILE = \"last_offset.json\"\n",
        "\n",
        "# API endpoint\n",
        "# Go to https://reliefweb.int/updates to crate your query,\n",
        "# use the API converted to convert your query --> https://reliefweb.int/search/converter?appname=owl1.0&search-url=https%3A%2F%2Freliefweb.int%2Fupdates%3Fadvanced-search%3D%2528L267%2529_%2528DA20160101-20250131%2529&submit=Convert\n",
        "URL = \"https://api.reliefweb.int/v1/reports?appname=owl1.0&profile=list&preset=latest&slim=1&query%5Bvalue%5D=date.created%3A%5B2016-01-01+TO+2025-01-31%7D+AND+language.id%3A267&query%5Boperator%5D=AND\"\n",
        "\n",
        "# Load last saved offset\n",
        "if os.path.exists(OFFSET_FILE):\n",
        "    with open(OFFSET_FILE, \"r\") as file:\n",
        "        last_offset = json.load(file).get(\"last_offset\", 0)\n",
        "else:\n",
        "    last_offset = 0  # Start from the beginning if no offset file exists\n",
        "\n",
        "# API parameters (Set limit to 10 for testing)\n",
        "params = {\n",
        "    'appname': 'app_1',\n",
        "    'limit': 10,  #  Testing with 10 records per request\n",
        "    'offset': last_offset,  # Resume from last fetched offset\n",
        "    'sort[]': ['date.created:asc'],  # Fetch incrementally\n",
        "    'fields[include][]': [\n",
        "        \"id\", \"title\", \"body\", \"country\", \"date.changed\", \"date.created\",\n",
        "        \"date.original\", \"disaster\", \"feature.name\", \"file\", \"format\", \"headline\",\n",
        "        \"language\", \"ocha_product\", \"origin\", \"primary_country.name\",\n",
        "        \"primary_country.iso3\", \"source\", \"status\", \"theme.name\", \"url_alias\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Fetch reports in batches\n",
        "new_reports = []\n",
        "request_count = 0  # Track daily API requests\n",
        "max_requests = 5  # Limit the number of requests for testing (adjust as needed)\n",
        "\n",
        "while request_count < max_requests:  # Stop after max_requests for testing\n",
        "    response = requests.get(URL, params=params)\n",
        "    request_count += 1  # Count API calls\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error fetching data: {response.status_code}\")\n",
        "        break\n",
        "\n",
        "    data = response.json()\n",
        "    reports = data.get('data', [])\n",
        "\n",
        "    if not reports:\n",
        "        print(\"No new reports available.\")\n",
        "        break\n",
        "\n",
        "    new_reports.extend(reports)\n",
        "\n",
        "    # Extract new offset info\n",
        "    current_offset = data.get('stats', {}).get('current_offset', params['offset'])\n",
        "    remaining_reports = data.get('stats', {}).get('remaining_reports', 0)\n",
        "\n",
        "    # Update offset for next request\n",
        "    params['offset'] = current_offset + params['limit']\n",
        "\n",
        "    print(f\"Fetched {len(reports)} new reports. Remaining: {remaining_reports}\")\n",
        "\n",
        "    # Stop if no more reports are available\n",
        "    if remaining_reports <= 0:\n",
        "        break\n",
        "\n",
        "# Convert new reports to DataFrame\n",
        "if new_reports:\n",
        "    df = pd.json_normalize(new_reports)\n",
        "    #display(df.head())\n",
        "    print('files fetched')\n",
        "\n",
        "    # Save last offset to resume next time\n",
        "    with open(OFFSET_FILE, \"w\") as file:\n",
        "        json.dump({\"last_offset\": params['offset']}, file)\n",
        "\n",
        "    print(f\"Successfully fetched {len(new_reports)} new reports. Next offset: {params['offset']}\")\n",
        "else:\n",
        "    print(\"No new reports to fetch today.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt57xjSaRKPH",
        "outputId": "30e2495f-7543-44f2-8ae2-15fea66c4e21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched 10 new reports. Remaining: 0\n",
            "files fetched\n",
            "âœ… Successfully fetched 10 new reports. Next offset: 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report_data=df[['id', 'fields.title', 'fields.body', 'fields.country',  'fields.date.changed', 'fields.date.created', 'fields.date.original', 'fields.disaster', 'fields.file', 'fields.format',\n",
        "   'fields.language','fields.origin', 'fields.primary_country.name', 'fields.primary_country.iso3', 'fields.source','fields.theme', 'fields.status',  'fields.url_alias']].copy()\n",
        "report_data.rename(columns={col: col.replace(\"fields.\", \"\") for col in df.columns}, inplace=True)\n",
        "report_data.rename(columns={'primary_country.name': 'primary_country_name'}, inplace=True)\n",
        "report_data.rename(columns={'primary_country.iso3': 'primary_country_iso3'}, inplace=True)\n",
        "report_data['country'] = report_data['country'].apply(lambda x: x[0]['name'] if isinstance(x, list) and x else None)\n",
        "report_data['filename']= report_data['file'].apply(lambda x: x[0]['filename'] if isinstance(x, list) and x else None)\n",
        "report_data['file'] = report_data['file'].apply(lambda x: x[0]['url'] if isinstance(x, list) and x else None)\n",
        "report_data['format'] = report_data['format'].apply(lambda x: x[0]['name'] if isinstance(x, list) and x else None)\n",
        "report_data['language'] = report_data['language'].apply(lambda x: x[0]['name'] if isinstance(x, list) and x else None)\n",
        "report_data['source'] = report_data['language'].apply(lambda x: x[0]['shortname'] if isinstance(x, list) and x else None)\n",
        "report_data['theme'] = report_data['theme'].apply(lambda x: [d.get('name', '') for d in x] if isinstance(x, list) else [])"
      ],
      "metadata": {
        "id": "pIvmEE1A4MwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_to_ndjson(input_df, output_ndjson_path):\n",
        "    # Convert the entire DataFrame to a list of dictionaries at once\n",
        "    records = input_df.to_dict('records')\n",
        "\n",
        "    # Open the NDJSON file for writing\n",
        "    with open(output_ndjson_path, 'w') as ndjson_file:\n",
        "        # Iterate over the list of dictionaries and write each as a JSON object\n",
        "        for record in records:\n",
        "            json.dump(record, ndjson_file)\n",
        "            ndjson_file.write('\\n')  # Add a newline after each JSON object"
      ],
      "metadata": {
        "id": "x8Doa44gaf3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "today_date = pd.to_datetime('today').strftime('%Y-%m-%d')\n",
        "output_path = f'analysis_pages_{today_date}.ndjson'\n",
        "df_to_ndjson(report_data, output_path)\n"
      ],
      "metadata": {
        "id": "zsbxjjfCapGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# _# Load the NDJSON file into a DataFrame\n",
        "# df = pd.read_json(output_path, lines=True)\n",
        "# df"
      ],
      "metadata": {
        "id": "Xd6__pIfajVz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}